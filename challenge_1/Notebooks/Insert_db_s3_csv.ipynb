{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database Globant as user admin\n",
      "Connected to AWS S3 in us-east-1 region\n",
      "insumo/\n",
      "insumo/departments.csv\n",
      "insumo/hired_employees.csv\n",
      "insumo/jobs.csv\n",
      "Inserting\n",
      "Inserting\n",
      "Inserting\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "from typing import Tuple\n",
    "import re\n",
    "import os\n",
    "import botocore\n",
    "from io import StringIO\n",
    "\n",
    "def connect_aws(aws_access_key_id: str, aws_secret_access_key: str, aws_region_name: str):\n",
    "    try:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id,\n",
    "                                    aws_secret_access_key=aws_secret_access_key,\n",
    "                                    region_name=aws_region_name)\n",
    "        print(f\"Connected to AWS S3 in {aws_region_name} region\")\n",
    "        return s3\n",
    "    except botocore.exceptions.NoCredentialsError:\n",
    "        print(\"AWS credentials not found or invalid.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(f\"Failed to connect to AWS S3: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while connecting to AWS S3: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def connect_db(user: str, password: str, host: str, port: str, db_name: str) -> Tuple:\n",
    "    \"\"\"Conexión a la base de datos\n",
    "\n",
    "    Args:\n",
    "        user (str): usuario de la base de datos\n",
    "        password (str): contraseña del usuario\n",
    "        host (str): dirección IP o hostname del servidor de la base de datos\n",
    "        port (str): puerto del servidor de la base de datos\n",
    "        db_name (str): nombre de la base de datos a la que conectarse\n",
    "\n",
    "    Returns:\n",
    "        Tuple: devuelve dos objetos para manejar la conexión con la base de datos:\n",
    "            sql_engine (sqlalchemy.engine.base.Engine): objeto para utilizarlo como conexión y así, guardar información a la base de datos\n",
    "            db_connection (sqlalchemy.engine.base.Connection): objeto para utilizarlo como conexión y así, leer información de la base datos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "        sql_engine = create_engine(db_url)\n",
    "        db_connection = sql_engine.connect()\n",
    "        print(f\"Connected to database {db_name} as user {user}\")\n",
    "    except Exception as e:\n",
    "        sql_engine = None\n",
    "        db_connection = None\n",
    "        print(f\"Failed to connect to database {db_name} as user {user}: {e}\")\n",
    "    return sql_engine, db_connection\n",
    "\n",
    "\n",
    "\n",
    "def get_name_files(s3,bucket: str,prefix : str)-> list:\n",
    "    \"\"\"\n",
    "    Obtiene los nombres de los archivos en una carpeta específica de un bucket de S3.\n",
    "\n",
    "    Args:\n",
    "        s3: objeto de la clase boto3.client para interactuar con S3.\n",
    "        bucket (str): nombre del bucket.\n",
    "        prefix (str): prefijo para filtrar los objetos del bucket.\n",
    "\n",
    "    Returns:\n",
    "        Una lista con los nombres de los archivos en la carpeta especificada.\n",
    "    \"\"\" \n",
    "        # Reemplaza 'my-bucket' y 'my-folder' con el nombre de tu bucket y carpeta, respectivamente\n",
    "    response = s3.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        # Obtiene los nombres de los archivos en la carpeta especificada\n",
    "    files = []\n",
    "    for content in response.get('Contents', []):\n",
    "        print(content['Key'])\n",
    "        file_name = re.sub('\\.csv$', '', content['Key'])\n",
    "        files.append(file_name)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_s3_files(bucket: str, pattern: str, aws_access_key_id: str, aws_secret_access_key: str,s3)-> Tuple:\n",
    "    \"\"\"Obtiene los archivos CSV del bucket de Amazon S3 especificado que cumplen el patrón especificado.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): nombre del bucket de Amazon S3\n",
    "        pattern (str): patrón para buscar los archivos\n",
    "        aws_access_key_id (str): AWS access key ID\n",
    "        aws_secret_access_key (str): AWS secret access key\n",
    "\n",
    "    Returns:\n",
    "        list: lista de archivos CSV que cumplen el patrón\n",
    "    \"\"\"\n",
    "    table_names = get_name_files(s3,bucket,pattern)\n",
    "    session = boto3.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "    s3_resource = session.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket)\n",
    "    csv_files = []\n",
    "    for obj in bucket.objects.filter(Prefix=pattern):\n",
    "        key = obj.key\n",
    "        if key.endswith('.csv'):\n",
    "            csv_file = obj.get()['Body'].read().decode('utf-8')\n",
    "            csv_files.append(pd.read_csv(StringIO(csv_file),header=None))\n",
    "    \n",
    "    table_names_final = [elemento for elemento in table_names if elemento != 'insumo/']\n",
    "    table_names_final = [elemento.replace('insumo/', '') for elemento in table_names_final]\n",
    "    return csv_files,table_names_final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def insert_to_db(sql_engine, s3_csv_files_data: list,s3_files_names: list) -> None:\n",
    "    \"\"\"Inserta los datos del archivo CSV en la tabla correspondiente de la base de datos.\n",
    "\n",
    "    Args:\n",
    "        sql_engine (sqlalchemy.engine.base.Engine): objeto para utilizarlo como conexión y así, guardar información a la base de datos\n",
    "        table_name (str): nombre de la tabla de la base de datos\n",
    "        s3_files_names (str): data de cada archivo\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    column_names = [\n",
    "        # Sublista para la tabla \"departments\"\n",
    "        [\"id\", \"department\"],\n",
    "        # Sublista para la tabla \"hired_employees\"\n",
    "        [\"id\", \"name\", \"datetime\", \"department_id\", \"job_id\"],\n",
    "        # Sublista para la tabla \"jobs\"\n",
    "        [\"id\", \"job\"]\n",
    "    ]\n",
    "    for i in range(len(s3_files_names)):\n",
    "        # Insert into database\n",
    "                            #df\n",
    "            try:\n",
    "                s3_csv_files_data[i].columns = column_names[i]\n",
    "                s3_csv_files_data[i].to_sql(name=s3_files_names[i], con=sql_engine, if_exists=\"append\", index=None)\n",
    "                print(\"Inserting\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to insert rows from {s3_files_names[i]}.csv into table {s3_files_names[i]}: {e}\")\n",
    "                raise e\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aws_access_key_id = 'AKIA4EUEBZDHFV3BYTMI'\n",
    "    aws_secret_access_key = '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'\n",
    "\n",
    "    aws_region_name = 'us-east-1'\n",
    "    s3_bucket_name = 'info-globant'\n",
    "    s3_prefix = 'insumo'\n",
    "\n",
    "    # Database credentials\n",
    "\n",
    "    user=\"admin\"\n",
    "    password=\"12345678\"\n",
    "    host=\"mydb.cjt7teobtbru.us-east-1.rds.amazonaws.com\"\n",
    "    port=\"3306\"\n",
    "    db=\"Globant\"\n",
    "\n",
    "\n",
    "\n",
    "    # Connect to the database\n",
    "    sql_engine, db_connection = connect_db(user, password, host, port, db)\n",
    "\n",
    "    #connect to aws\n",
    "    s3=connect_aws(aws_access_key_id,aws_secret_access_key,aws_region_name)\n",
    "\n",
    "    #  Get table names for each CSV file and get dfs in a list\n",
    "    s3_csv_files_data,s3_files_names = get_s3_files(s3_bucket_name, s3_prefix,aws_access_key_id,aws_secret_access_key,s3)\n",
    "\n",
    "    insert_to_db(sql_engine, s3_csv_files_data,s3_files_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['departments', 'hired_employees', 'jobs']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if __name__ == \"__main__\":\n",
    "        aws_access_key_id = 'AKIA4EUEBZDHFV3BYTMI'\n",
    "        aws_secret_access_key = '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'\n",
    "\n",
    "        aws_region_name = 'us-east-1'\n",
    "        s3_bucket_name = 'info-globant'\n",
    "        s3_prefix = 'insumo'\n",
    "\n",
    "        # Database credentials\n",
    "\n",
    "        db_user=\"admin\"\n",
    "        db_password=\"12345678\"\n",
    "        db_host=\"mydb.cjt7teobtbru.us-east-1.rds.amazonaws.com\"\n",
    "        db_port=\"3306\"\n",
    "        db_name=\"Globant\"\n",
    "\n",
    "        # CSV parameters\n",
    "        microbatch = 20\n",
    "\n",
    "        # Connect to the database\n",
    "        sql_engine, db_connection = connect_db(db_user, db_password, db_host, db_port, db_name)\n",
    "\n",
    "        #connect to aws\n",
    "        s3=connect_aws(aws_access_key_id,aws_secret_access_key,aws_region_name)\n",
    "\n",
    "        #  Get table names for each CSV file and get dfs in a list\n",
    "        s3_csv_files_data,s3_files_names = get_s3_files(s3_bucket_name, s3_prefix,aws_access_key_id,aws_secret_access_key,s3)\n",
    "\n",
    "        insert_to_db(sql_engine, s3_csv_files_data,s3_files_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # AWS S3 credentials\n",
    "    aws_access_key_id = 'AKIA4EUEBZDHFV3BYTMI'\n",
    "    aws_secret_access_key = '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'\n",
    "\n",
    "    aws_region_name = 'us-east-1'\n",
    "    s3_bucket_name = 'info-globant'\n",
    "    s3_prefix = 'departments.csv'\n",
    "\n",
    "    # Database credentials\n",
    "\n",
    "    db_user=\"admin\"\n",
    "    db_password=\"12345678\"\n",
    "    db_host=\"mydb.cjt7teobtbru.us-east-1.rds.amazonaws.com\"\n",
    "    db_port=\"3306\"\n",
    "    db_name=\"Globant\"\n",
    "\n",
    "    # CSV parameters\n",
    "    microbatch = 20\n",
    "\n",
    "    # Connect to the database\n",
    "    sql_engine, db_connection = connect_db(db_user, db_password, db_host, db_port, db_name)\n",
    "\n",
    "    # Connect to S3\n",
    "    s3_client = connect_aws(aws_access_key_id=aws_access_key_id,\n",
    "                             aws_secret_access_key=aws_secret_access_key,\n",
    "                             region_name=aws_region_name)\n",
    "\n",
    "    # Get list of CSV files in S3 bucket\n",
    "    s3_csv_files = get_s3_csv_files(s3_client, s3_bucket_name, s3_prefix)\n",
    "\n",
    "    # Get table names for each CSV file\n",
    "    table_names = get_table_names(s3_csv_files)\n",
    "\n",
    "    # Insert CSV data into database\n",
    "    for i in range(len(s3_csv_files)):\n",
    "        s3_csv_file = s3_csv_files[i]\n",
    "        table_name = table_names[i]\n",
    "        csv_data = get_csv_data_from_s3(s3_client, s3_bucket_name, s3_csv_file)\n",
    "        insert_csv_to_db(microbatch, csv_data, sql_engine, table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Globant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
